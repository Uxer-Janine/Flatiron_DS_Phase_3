{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf80bae-15dc-4dc3-aeeb-ce6606944b60",
   "metadata": {},
   "source": [
    "Lesson Layout:\n",
    "- Recap Linear regression\n",
    "- Polynomial Regression\n",
    "- Interaction Terms\n",
    "- Significance of interaction effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67247b12-5799-4a2d-acec-77cf31648118",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 0. Recap: Linear Regression\n",
    "\n",
    "Simple linear regression models the relationship between one independent variable (X) and one dependent variable (Y) using the equation:  \n",
    "\\( Y = β₀ + β₁X + ε \\)  \n",
    "where \\( β₀ \\) is the intercept, \\( β₁ \\) is the slope, and \\( ε \\) is the error term.\n",
    "\n",
    "Multiple linear regression extends this to multiple predictors:  \n",
    "\\( Y = β₀ + β₁X₁ + β₂X₂ + \\dots + βₙXₙ + ε \\)\n",
    "\n",
    "Key assumptions include:\n",
    "- Linearity between predictors and response\n",
    "- Independence of observations\n",
    "- Homoscedasticity (constant variance of errors)\n",
    "- Normal distribution of residuals\n",
    "- No multicollinearity among predictors\n",
    "\n",
    "Common evaluation metrics:\n",
    "- R²: explains the proportion of variance in Y\n",
    "- MSE / RMSE: measure average prediction error\n",
    "- MAE: mean absolute error, less sensitive to outliers\n",
    "\n",
    "Use cases include price prediction, trend estimation, and identifying variable relationships.\n",
    "\n",
    "Limitations: can't handle non-linear relationships well and is sensitive to outliers.\n",
    "\n",
    "> *A quick code along on linear regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df0a2d-b0fa-44cc-8ee3-a17bbe7e3f61",
   "metadata": {},
   "source": [
    "# 1. Polynomial Regression\n",
    "\n",
    "Polynomial regression and interaction terms expand the toolkit for modeling complex relationships in multiple linear regression. These techniques capture nonlinear patterns and joint effects between variables, allowing for more accurate and nuanced analyses of real-world data.\n",
    "\n",
    "By incorporating higher-order terms and interactions, we can uncover hidden relationships and improve model fit. Understanding these concepts is crucial for making informed decisions about model specification and interpreting results accurately.\n",
    "\n",
    "## Non-Linear Relationships in Regression.\n",
    "\n",
    "- Nonlinear relationships occur when the change in the independet variable is not proportional to the change in the predictor variable\n",
    "- Scatterplots can visually reveal nonlinear patterns (curves or bends) indicating that a linear model may not adequately capture the relationship between the predictor and response variables\n",
    "- Common nonlinear patterns include:\n",
    "  > *Lead Notes | Draw sample quadratic and exponential charts here* \n",
    "    - Quadratic (U-shaped or inverted U-shaped)\n",
    "    - Exponential (rapidly increasing or decreasing)\n",
    "    - Logarithmic (rapid change followed by a leveling off)\n",
    "- Residual plots can also help identify nonlinear relationships by showing a systematic pattern in the residuals when a linear model is fitted to nonlinear data\n",
    "\n",
    "## Consequences of ignoring nonlinear relationships\n",
    "\n",
    "Ignoring nonlinear relationships and using a linear model can lead to:\n",
    "- Biased estimates\n",
    "- Inaccurate predictions\n",
    "- Incorrect conclusions about the relationship between the predictor and response variables\n",
    "\n",
    "Fitting a linear model to nonlinear data can result in a poor fit and misleading interpretations of the relationship between variables.\n",
    "\n",
    "Nonlinear relationships require alternative modeling approaches (polynomial regression, transformations, or non-parametric methods) to accurately capture the underlying pattern and make valid inferences\n",
    "\n",
    "## Polynomial Regression\n",
    "\n",
    "Polynomial regression models capture **nonlinear relationships** between predictors and the response variable by including **higher-order terms** (squared, cubed, etc.) of the predictors in the model.\n",
    "\n",
    "The general form of a polynomial regression model is:\n",
    "$$\n",
    "Y = β₀ + β₁X + β₂X² + \\dots + βₚXᵖ + ε\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- \\( Y \\) is the response variable  \n",
    "- \\( X \\) is the predictor  \n",
    "- \\( β₀, β₁, \\dots, βₚ \\) are the coefficients  \n",
    "- \\( p \\) is the degree of the polynomial  \n",
    "- \\( ε \\) is the error term  \n",
    "\n",
    "The **quadratic model** (\\( p = 2 \\)) is the most common form of polynomial regression and includes a squared term of the predictor variable:\n",
    "\n",
    "$$\n",
    "Y = β₀ + β₁X + β₂X² + ε\n",
    "$$\n",
    "\n",
    "Higher-order polynomial terms (e.g., cubic, quartic) can be added to capture more **complex nonlinear relationships**. However, this comes with the **risk of overfitting**, especially as the model complexity increases.\n",
    "\n",
    "## Interpretation of polynomial regression coefficients\n",
    "\n",
    "Polynomial regression models are still considered linear models because they are linear in the parameters ($β₀$, $β₁$, $β₂$, etc.), even though they capture nonlinear relationships between the predictors and the response variable\n",
    "\n",
    "The interpretation of the coefficients in a polynomial regression model depends on the degree of the polynomial and the presence of lower-order terms\n",
    "\n",
    "In a quadratic model:\n",
    "- $β₀$ represents the intercept or the expected value of $Y$ when $X = 0$\n",
    "- $β₁$ represents the linear effect of $X$ on $Y$, holding the quadratic term constant\n",
    "- $β₂$ represents the quadratic effect of $X$ on $Y$, indicating the rate of change in the linear effect as $X$ increases\n",
    "\n",
    "The significance of the polynomial terms can be assessed using hypothesis tests and p-values, helping to determine the appropriate degree of the polynomial model\n",
    "\n",
    "> *Student Code Along on*\n",
    "> *Polynomial Regression*\n",
    "> *Diagonise Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba73b2-2e66-4f61-8a6e-38c062830cc0",
   "metadata": {},
   "source": [
    "# 2. Interaction Terms in Regression\n",
    "\n",
    "## Understanding interaction effects\n",
    "\n",
    "Interaction terms in a multiple regression model capture the joint effect of two or more predictor variables on the response variable, beyond their individual effects\n",
    "\n",
    "An interaction term is created by multiplying two or more predictor variables:\n",
    "- $Y = β₀ + β₁X₁ + β₂X₂ + β₃(X₁ × X₂) + ε$, where $X₁ × X₂$ is the interaction term\n",
    "\n",
    "The coefficient of the interaction term ($β₃$) represents the change in the effect of one predictor variable on the response variable for a one-unit change in the other predictor variable\n",
    "\n",
    "When an interaction term is significant, the interpretation of the main effects ($β₁$ and $β₂$) becomes conditional on the value of the other predictor variable involved in the interaction\n",
    "\n",
    "## Interpreting and visualizing interaction effects\n",
    "\n",
    "The presence of a significant interaction indicates that the effect of one predictor variable on the response variable depends on the level of the other predictor variable\n",
    "\n",
    "Interaction plots (or simple slopes analysis) can help visualize and interpret the nature of the interaction effect by showing the relationship between one predictor and the response variable at different levels of the other predictor\n",
    "\n",
    "> Example: In a study examining the effect of study time and IQ on exam scores, a significant interaction between study time and IQ would suggest that the effect of study time on exam scores varies depending on the student's IQ level\n",
    "\n",
    "Simple slopes analysis can quantify the effect of one predictor on the response variable at specific levels (low, medium, high) of the other predictor involved in the interaction\n",
    "\n",
    "> Code Along on Interaction Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e5abb-b1e2-4756-9d21-38fd1e32f98d",
   "metadata": {},
   "source": [
    "# 3. Significance of interaction effects \n",
    "\n",
    "## Assessing statistical significance\n",
    "The significance of an interaction effect is determined by the p-value associated with the coefficient of the interaction term ($β₃$) in the multiple regression model\n",
    "\n",
    "A small p-value (typically < 0.05) indicates that the interaction effect is statistically significant, suggesting that the joint effect of the predictor variables on the response variable is unlikely to have occurred by chance\n",
    "\n",
    "The statistical significance of an interaction effect provides evidence for the existence of a moderation effect, where the relationship between one predictor and the response variable depends on the level of another predictor\n",
    "\n",
    "\n",
    "## Practical implications and considerations\n",
    "\n",
    "The practical significance of an interaction effect depends on the magnitude of the coefficient and the context of the study, considering factors such as the units of measurement and the range of the predictor variables\n",
    "\n",
    "Standardized coefficients (beta weights) can be used to compare the relative importance of interaction effects across different predictors and studies\n",
    "\n",
    "The presence of a significant interaction effect can have important implications for the interpretation and application of the research findings, as it suggests that the relationship between the predictors and the response variable is more complex than simple main effects\n",
    "\n",
    "Ignoring significant interaction effects can lead to incorrect conclusions and suboptimal decisions, as the effect of one predictor on the response variable may vary depending on the level of another predictor\n",
    "\n",
    "When reporting and discussing interaction effects, it is crucial to provide a clear interpretation of the nature and direction of the interaction, along with any relevant simple slopes analysis or interaction plots\n",
    "\n",
    "> Example: In a marketing study investigating the effect of price and product quality on sales, a significant interaction between price and quality would imply that the optimal pricing strategy depends on the product's quality level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4457931-ae23-4757-8b9d-2e35ae57d6d1",
   "metadata": {},
   "source": [
    "Lesson Resources:\n",
    " - Quantile Quantile plots - https://library.virginia.edu/data/articles/understanding-q-q-plots\n",
    " - Residual plots - https://www.qualtrics.com/support/stats-iq/analyses/regression-guides/interpreting-residual-plots-improve-regression/\n",
    "\n",
    "Take away exercise:\n",
    "- https://www.kaggle.com/datasets/fratzcan/usa-house-prices?select=USA+Housing+Dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a59546e-f865-420a-a6e9-a0188cca60fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
